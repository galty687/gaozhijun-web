---
layout: post
title: 大模型基础知识
excerpt: "区分与辨析常见术语和参数"
modified: 
tags: [LLM, GPT]
comments: true
category: AIGC
---



## 术语

- Laguage Model
- Large Laguage Model
- GPT
- LLM





## 概念

### Quantization

大模型的特点就是非常大，主要是因为大模型的底层深度神经网络有大量的参数和权重，GPT2 有 15亿参数，如果用32位的浮点数值，则需要 6 GB的内存（ `4 bytes * 1,500,000,000 = 6GB RAM` ）。最新发布的LLaMa2，650亿参数的大模型，则需要260GB的内存，而且算上持续性的输入和输出，还需要更多内存，因此一般的个人计算机是无法运行大模型的。

在模型推理过程中，RAM和CPU之间的带宽通常是性能瓶颈，而不是处理核心的数量或速度。这是因为处理器在执行任务时可能会缺乏足够的数据。一种缓解这个问题的方法是使用更小的数据类型，如16位浮点数，以减少RAM使用量和内存带宽。NVIDIA的最新硬件开始支持bfloat16数据类型，该类型保留了float32的完整指数范围，但牺牲了2/3的精度。研究表明，这种做法在质量和性能之间达到了良好的平衡，而且模型对于数据精度的损失并不特别敏感。



| Format   | Significand | Exponent |
| -------- | ----------- | -------- |
| bfloat16 | 8 bits      | 8 bits   |
| float16  | 11 bits     | 5 bits   |
| float32  | 24 bits     | 8 bits   |

在 torch_dtype 中，默认是 `float32`，但也可以指定 `float16` 或 `bfloat16`

虽然可以进一步从16位降低到8位或4位，但这样做通常不支持硬件加速的浮点运算。为了获得硬件加速，可以使用较小的整数和向量化指令集。比如，Intel的AVX（Advanced Vector Extensions）指令集可以对整数进行硬件加速，从而实现快速的向量运算。

一个简单的量化方法是“后训练量化”（Post-training quantization）。具体来说，你首先找到模型权重的最大值和最小值，然后根据你的整数类型所能表达的范围（例如，8位整数有256个不同的值，4位整数有16个不同的值）将这个范围划分为若干个“桶”（buckets）。然后，你可以将模型权重的浮点数值映射到这些桶中最近的整数值。

通过这种方式，即使模型最初是用32位浮点数进行训练的，也可以将其转换为更小的整数格式，同时仍然能够从硬件加速的指令集中受益。这不仅可以减少模型的大小，还可以加速其推理过程，尤其是在资源受限的设备上。

在深度学习和机器学习模型中，为了加速计算和减少存储需求，经常会对模型进行量化。量化就是将模型的**浮点数权重**和激活值转换为更小的**整数**或**定点数**。这个过程通常会降低模型的计算复杂性，提高模型在边缘设备上的性能。

## 模型类型

### GGML

GGML 是一个库，专门用于运行机器学习模型。库（Library）在编程中是一组预编译的代码，这些代码可以被多个程序共用。通过使用库，开发者可以避免从零开始编写代码，从而提高开发速度和效率。

GGML 库的显著特点是在 CPU 上能够高效运行。传统上，这种机器学习模型通常是在 GPU（图形处理器）上运行的，因为 GPU 在并行计算方面非常强大，适合进行大量的数学运算。然而，拥有大量内存的专用 GPU 通常非常昂贵。相比之下，GGML 能够在普通的硬件上达到可接受的运行速度。

这里的“普通硬件”（commodity hardware）通常指的是成本相对较低、易于获取的计算资源，与专门定制或者是高端硬件相对。

### GPTQ
GPT Quantization 来自这篇论文：[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)

生成预训练变换器模型（Generative Pre-trained Transformer），简称为 GPT 或 OPT，通过在复杂的语言建模任务中取得突破性的表现而脱颖而出。但这些模型也存在极高的计算和存储成本。特别是，由于模型规模巨大，即使是用于大型、高精度 GPT 模型的推断也可能需要多个高性能 GPU，这限制了这类模型的可用性。尽管有针对缓解这种压力的模型压缩工作正在进行，但现有压缩技术的适用性和性能受到 GPT 模型的规模和复杂性的限制。

在这篇论文中，我们提出了 GPTQ，这是一种基于近似二阶信息的新型一次性权重量化方法，具有高精度和高效率。具体来说，GPTQ 能在大约四个 GPU 小时内量化拥有 1750 亿参数的 GPT 模型，将每个权重的比特宽度减少到 3 或 4 比特，同时相对于未压缩的基线几乎不损失精度。与之前提出的一次性量化方法相比，我们的方法将压缩效益提高了一倍多，同时保留了精度，这使我们首次能够在单个 GPU 内执行含有 1750 亿参数的模型进行生成推断。

此外，我们还展示了在极端量化情况下，我们的方法仍然能提供合理的准确度，即权重被量化为 2 位甚至三元量化水平。我们通过实验表明，使用这些改进可以实现端到端推断速度的提升，在使用高端 GPU（NVIDIA A100）时大约是 3.25 倍，在使用更具成本效益的 GPU（NVIDIA A6000）时为 4.5 倍。

总体来说，GPTQ 通过一次性权重量化方法成功地降低了 GPT 模型的计算和存储成本，同时基本上没有损失准确性，这在一定程度上解决了 GPT 模型的可用性问题。与 GGML 相比，GPTQ 更专注于针对特定（尤其是大型）模型的优化，能够在单一 GPU 中运行更大规模的模型，还能在高端和成本效益更高的 GPU 上实现更快的推断速度。

### GGUF



### HF

### GGUF

### 模型选用推荐

最佳性能：如果你追求最高性能，选择配备高端 GPU（如 NVIDIA 的最新 RTX 3090 或 RTX 4090）或双 GPU 设置的机器，以适应最大的模型（65B 和 70B）。拥有足够 RAM（最低 16 GB，最佳为 64 GB）的系统将是最优选择。

预算限制：如果你的预算有限，可以集中关注适合在系统 RAM 内运行的 WizardLM GGML/GGUF 模型。请记住，虽然你可以将一些权重卸载到系统 RAM 中，但这将会以性能为代价。

CPU要求

为了获得最佳性能，建议使用现代多核CPU。从第8代开始的Intel Core i7或从第3代开始的AMD Ryzen 5都是不错的选择。具有6核或8核的CPU是理想的。更高的时钟速度也会提高命令处理速度，因此建议选择3.6GHz或更高的速度。

如果可用，拥有CPU指令集，如AVX、AVX2、AVX-512，可以进一步提高性能。关键是要有一个相当现代的消费级CPU，具有不错的核心数量和时钟速度，以及至少达到AVX2级别的基线向量处理能力（这对于使用llama.cpp进行CPU推断是必需的）。具备这些规格的CPU应该能够很好地处理WizardLM模型大小。

## 常见大模型

### 中文

- [Baichuan](https://github.com/baichuan-inc/Baichuan-13B)
- [Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese)

## 大模型微调

### 备忘项目

- [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/main/README_zh.md)
- [Instruction Tuning with GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | [项目说明](https://zhuanlan.zhihu.com/p/645010851)






## 参考

- [Announcing GPTQ & GGML Quantized LLM support for Huggingface Transformers](https://postgresml.org/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers)
- 

