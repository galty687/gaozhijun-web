---
layout: post
title: Dify 使用笔记
excerpt: "dify使用的记录"
modified: 
tags: [LLM, Chatgpt]
comments: true
category: AIGC

---



## 从Docker中调用本地大模型



故障：直接输入本地化大模型localhost就是会一直连不上

原因：默认情况下，容器内部无法直接访问宿主机的网络服务。因此，需要一种方式来使容器内部的应用程序能够访问宿主机提供的服务。

解决：http://host.docker.internal:11434

>`http://host.docker.internal` 是一个特殊的 DNS 名称，在 Windows 和 MacOS 上的 Docker Desktop 提供，用于让容器访问宿主机的网络服务。这是 Docker 提供的一种便捷方式，使容器内部的应用程序能够通过这个地址访问宿主机。



## ollama运行本地大模型

官网：https://ollama.com



## 常见命令

```Terminal
# 显示本地已经安装的大模型
ollama list

# 运行本地大模型
ollama qwen2:7b



```



## 更新Dify

Dify隔一段时间就会更新，目前是一更新就会失去所有的配置，信息

```Terminal
# 复制最新Dify 到本地化
git clone https://github.com/langgenius/dify.git


# 升级指令
cd dify/docker
docker compose down
git pull origin main
docker compose pull
docker compose up -d

```



# 工作流

## 先机器翻译再润色

![workflow](assets/blog-images/2024/08/workflow.png)





导出的DSL文件

```
app:
  description: ''
  icon: 🤖
  icon_background: '#FFEAD5'
  mode: workflow
  name: smartrans
kind: app
version: 0.1.1
workflow:
  conversation_variables: []
  environment_variables: []
  features:
    file_upload:
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInIteration: false
        sourceType: start
        targetType: tool
      id: 1723860987072-source-1723861027680-target
      source: '1723860987072'
      sourceHandle: source
      target: '1723861027680'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        sourceType: tool
        targetType: llm
      id: 1723861027680-source-1723861128111-target
      source: '1723861027680'
      sourceHandle: source
      target: '1723861128111'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInIteration: false
        sourceType: llm
        targetType: end
      id: 1723861128111-source-1723861135345-target
      source: '1723861128111'
      sourceHandle: source
      target: '1723861135345'
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        desc: Please input the text to translate
        selected: false
        title: Start
        type: start
        variables:
        - label: Text to Translate
          max_length: null
          options: []
          required: true
          type: paragraph
          variable: text2translate
      height: 118
      id: '1723860987072'
      position:
        x: 80
        y: 282
      positionAbsolute:
        x: 80
        y: 282
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        provider_id: google_translate
        provider_name: google_translate
        provider_type: builtin
        selected: false
        title: Translate
        tool_configurations:
          dest: zh-cn
        tool_label: Translate
        tool_name: translate
        tool_parameters:
          content:
            type: mixed
            value: '{{#1723860987072.text2translate#}}'
        type: tool
      height: 90
      id: '1723861027680'
      position:
        x: 384
        y: 282
      positionAbsolute:
        x: 384
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        context:
          enabled: false
          variable_selector: []
        desc: ''
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: gpt-4o
          provider: azure_openai
        prompt_template:
        - id: 50e227d0-4370-4f08-8daa-2bf8403fef89
          role: system
          text: '请对照原文对译文进行润色，最后输润色后的译文。


            原文：

            {{#1723860987072.text2translate#}}


            译文：

            {{#1723861027680.text#}}

            '
        selected: false
        title: LLM
        type: llm
        variables: []
        vision:
          configs:
            detail: high
          enabled: true
      height: 98
      id: '1723861128111'
      position:
        x: 689
        y: 282
      positionAbsolute:
        x: 689
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    - data:
        desc: ''
        outputs:
        - value_selector:
          - '1723861128111'
          - text
          variable: translation
        selected: false
        title: End
        type: end
      height: 90
      id: '1723861135345'
      position:
        x: 992
        y: 282
      positionAbsolute:
        x: 992
        y: 282
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 244
    viewport:
      x: 59.514009473272836
      y: 15.001932860512056
      zoom: 1.0482624755169463

```



## 长文档生成器

让ChatGPT直接写长文档，ChatGPT比较难写很长的文档。

解决思路：

接受用户输入文章标题和提纲，然后由LLM基于标题和大纲，生成更详细的二级提纲，然后将二级提纲交给LLM进行写作，写完后拼成一篇完整的文章。



难点：

1. 让大模型生成JSON格式
2. 将JSON复杂数据提取出所需内容后加工为词典
3. 迭代每个节点，让大模型就每个节点生成内容
4. 最后用模板输出为可读的文章





## 大模型基础

### 所需内存

以ollama 运行千问7b模型(qwen2:7b)为例：

7b也就是70亿参数，大模型运行的时候需要将模型参数全部装入内容，每个参数4字节的话，就是280亿个字节（约28G内存）。中间计算存储和框架开销也需要一些内存，综合来看：

- **推理阶段**：通常需要大约**40GB到50GB**的内存来运行一个7B参数的模型。

- **训练阶段**：训练所需的内存会更多，因为还需要存储优化器状态和梯度信息，通常需要两到三倍于推理阶段的内存。

### 量化

量化的过程通常涉及将模型参数从32位浮点数（FP32）转换为16位浮点数（FP16）或8位整数（INT8）。

- **FP32到FP16量化**：模型参数会从原来的32位浮点数压缩到16位浮点数，这将使内存需求减少一半。对于7B参数的模型，参数存储需求大约会从28GB减少到14GB。

- **FP32到INT8量化**：模型参数会从32位浮点数压缩到8位整数，这将使内存需求减少到四分之一。对于7B参数的模型：参数存储需求大约会从28GB减少到7GB。

- **FP32INT4量化**：模型参数会从32位浮点数压缩到4位整数，这将使内存需求减少到原来的八分之一。对于7B参数的模型，参数存储需求大约会从28GB减少到3.5GB。



72B大模型

| 量化方式     | 推理内存         |
| ------------ | ---------------- |
| **FP32模型** | 350GB到400GB内存 |
| **FP16量化** | 175GB到200GB内存 |
| **INT8量化** | 85GB到100GB内存  |



### 精度损失

**FP16量化**

- **优点**：相比FP32，FP16量化对精度的影响较小。大多数深度学习模型在转换到FP16后，性能几乎不变。
- **精度损失**：通常在1%以内，很多情况下可以忽略不计。
- **应用场景**：常用于训练和推理阶段，因为它能够在减少内存使用的同时保持高精度。

**INT8量化**

- **优点**：能够显著减少内存和计算资源需求。
- **精度损失**：在大多数情况下，精度损失在1%到5%之间。不过，对于某些敏感任务或模型，精度损失可能会更高。
- **应用场景**：主要用于推理阶段，特别是在资源受限的设备上，如移动设备和嵌入式系统。

**INT4量化**

- **优点**：进一步减少内存和计算资源需求，适用于极端资源受限的环境。
- **精度损失**：通常比INT8高，可能在5%到10%之间，有时甚至更多。对于某些任务，精度损失可能不可接受。
- **应用场景**：适用于对精度要求不高或可以容忍较大精度损失的任务。



### 速度

ollama 推理速度较快，是因为它运行的模型是经过 GGUF ( GPT-Generated Unified Format）方式量化的，从而减少模型的存储需求和计算量。这种方式允许用户使用CPU来运行LLM，也可以将某些层加载到GPU以提高速度。当然速度提升是以损失精度为代价的。

大模型的推理速度通常通过以下几个指标进行衡量：

**吞吐量（Throughput）**

- **定义**：单位时间内模型处理的样本数量，通常以每秒处理的样本数（samples per second）或每秒处理的标记数（tokens per second）来表示。
- **适用场景**：高吞吐量在需要快速处理大量数据的场景（如批量处理任务）中非常重要。

**延迟（Latency）**

- **定义**：处理单个输入所需的时间，通常以毫秒（ms）为单位。
- **适用场景**：低延迟在实时应用（如聊天机器人、在线翻译等）中非常关键，用户希望获得即时响应。

**每次推理的时间（Inference Time per Sample）**

- **定义**：处理单个输入样本从开始到结束所需的总时间。
- **适用场景**：在评估单个请求的处理时间时，这个指标很有用。

**内存使用（Memory Usage）**

- **定义**：推理过程中所需的内存量，通常以GB为单位。
- **适用场景**：在资源受限的环境（如移动设备或嵌入式系统）中，需要关注内存使用情况。

**计算成本（Compute Cost）**

- **定义**：推理过程中所需的计算资源，通常以FLOPs（每秒浮点运算次数）来表示。
- **适用场景**：评估计算资源的需求，特别是在大型集群或云计算环境中，计算成本会影响经济效益。



### 快速汇总

| **量化方法** | **内存需求减少** | **精度损失**      | **优点**                     | **应用场景**                             | **推理速度**   |
| ------------ | ---------------- | ----------------- | ---------------------------- | ---------------------------------------- | -------------- |
| **FP16**     | 约50%            | 通常在1%以内      | 减少内存使用，保持高精度     | 训练和推理阶段                           | 提高1.5倍到2倍 |
| **INT8**     | 约75%            | 1%到5%            | 显著减少内存和计算资源需求   | 主要用于推理，特别是在资源受限的设备上   | 提高2倍到4倍   |
| **INT4**     | 约87.5%          | 5%到10%，有时更高 | 进一步减少内存和计算资源需求 | 对精度要求不高或可容忍较大精度损失的任务 | 提高4倍到8倍   |

