---
layout: post
title: Dify 使用笔记
excerpt: "dify使用的记录"
modified: 
tags: [LLM, Chatgpt]
comments: true
category: AIGC

---



## 从Docker中调用本地大模型



故障：直接输入本地化大模型localhost就是会一直连不上

原因：默认情况下，容器内部无法直接访问宿主机的网络服务。因此，需要一种方式来使容器内部的应用程序能够访问宿主机提供的服务。

解决：http://host.docker.internal:11434

>`http://host.docker.internal` 是一个特殊的 DNS 名称，在 Windows 和 MacOS 上的 Docker Desktop 提供，用于让容器访问宿主机的网络服务。这是 Docker 提供的一种便捷方式，使容器内部的应用程序能够通过这个地址访问宿主机。



## ollama运行本地大模型

官网：https://ollama.com

### 所需内存

以ollama 运行千问7b模型(qwen2:7b)为例：

7b也就是70亿参数，大模型运行的时候需要将模型参数全部装入内容，每个参数4字节的话，就是280亿个字节（约28G内存）。中间计算存储和框架开销也需要一些内存，综合来看：

- **推理阶段**：通常需要大约**40GB到50GB**的内存来运行一个7B参数的模型。

- **训练阶段**：训练所需的内存会更多，因为还需要存储优化器状态和梯度信息，通常需要两到三倍于推理阶段的内存。

### 量化

量化的过程通常涉及将模型参数从32位浮点数（FP32）转换为16位浮点数（FP16）或8位整数（INT8）。

- **FP32到FP16量化**：模型参数会从原来的32位浮点数压缩到16位浮点数，这将使内存需求减少一半。对于7B参数的模型，参数存储需求大约会从28GB减少到14GB。

- **FP32到INT8量化**：模型参数会从32位浮点数压缩到8位整数，这将使内存需求减少到四分之一。对于7B参数的模型：参数存储需求大约会从28GB减少到7GB。

- **FP32INT4量化**：模型参数会从32位浮点数压缩到4位整数，这将使内存需求减少到原来的八分之一。对于7B参数的模型，参数存储需求大约会从28GB减少到3.5GB。



72B大模型

| 量化方式     | 推理内存         |
| ------------ | ---------------- |
| **FP32模型** | 350GB到400GB内存 |
| **FP16量化** | 175GB到200GB内存 |
| **INT8量化** | 85GB到100GB内存  |



### 精度损失

**FP16量化**

- **优点**：相比FP32，FP16量化对精度的影响较小。大多数深度学习模型在转换到FP16后，性能几乎不变。
- **精度损失**：通常在1%以内，很多情况下可以忽略不计。
- **应用场景**：常用于训练和推理阶段，因为它能够在减少内存使用的同时保持高精度。

**INT8量化**

- **优点**：能够显著减少内存和计算资源需求。
- **精度损失**：在大多数情况下，精度损失在1%到5%之间。不过，对于某些敏感任务或模型，精度损失可能会更高。
- **应用场景**：主要用于推理阶段，特别是在资源受限的设备上，如移动设备和嵌入式系统。

**INT4量化**

- **优点**：进一步减少内存和计算资源需求，适用于极端资源受限的环境。
- **精度损失**：通常比INT8高，可能在5%到10%之间，有时甚至更多。对于某些任务，精度损失可能不可接受。
- **应用场景**：适用于对精度要求不高或可以容忍较大精度损失的任务。



### 速度

ollama 推理速度较快，是因为它运行的模型是经过 GGUF ( GPT-Generated Unified Format）方式量化的，从而减少模型的存储需求和计算量。这种方式允许用户使用CPU来运行LLM，也可以将某些层加载到GPU以提高速度。当然速度提升是以损失精度为代价的。

大模型的推理速度通常通过以下几个指标进行衡量：

**吞吐量（Throughput）**

- **定义**：单位时间内模型处理的样本数量，通常以每秒处理的样本数（samples per second）或每秒处理的标记数（tokens per second）来表示。
- **适用场景**：高吞吐量在需要快速处理大量数据的场景（如批量处理任务）中非常重要。

**延迟（Latency）**

- **定义**：处理单个输入所需的时间，通常以毫秒（ms）为单位。
- **适用场景**：低延迟在实时应用（如聊天机器人、在线翻译等）中非常关键，用户希望获得即时响应。

**每次推理的时间（Inference Time per Sample）**

- **定义**：处理单个输入样本从开始到结束所需的总时间。
- **适用场景**：在评估单个请求的处理时间时，这个指标很有用。

**内存使用（Memory Usage）**

- **定义**：推理过程中所需的内存量，通常以GB为单位。
- **适用场景**：在资源受限的环境（如移动设备或嵌入式系统）中，需要关注内存使用情况。

**计算成本（Compute Cost）**

- **定义**：推理过程中所需的计算资源，通常以FLOPs（每秒浮点运算次数）来表示。
- **适用场景**：评估计算资源的需求，特别是在大型集群或云计算环境中，计算成本会影响经济效益。



### 快速汇总

| **量化方法** | **内存需求减少** | **精度损失**      | **优点**                     | **应用场景**                             | **推理速度**   |
| ------------ | ---------------- | ----------------- | ---------------------------- | ---------------------------------------- | -------------- |
| **FP16**     | 约50%            | 通常在1%以内      | 减少内存使用，保持高精度     | 训练和推理阶段                           | 提高1.5倍到2倍 |
| **INT8**     | 约75%            | 1%到5%            | 显著减少内存和计算资源需求   | 主要用于推理，特别是在资源受限的设备上   | 提高2倍到4倍   |
| **INT4**     | 约87.5%          | 5%到10%，有时更高 | 进一步减少内存和计算资源需求 | 对精度要求不高或可容忍较大精度损失的任务 | 提高4倍到8倍   |

